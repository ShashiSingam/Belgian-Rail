{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_pipeline_git.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz-ImUpkFk3r",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## ETL\n",
        "\n",
        "The ETL part consists in understanding the data structure and data types, creating the schema of the data and saving the data in a structured format, to be easily reused later, in the next steps of the project. \n",
        "\n",
        "Very often, you will receive new data, either an updated version of the data or the next batch of data (if temporal), so it is best that the code is clean and reusable hence when the next data comes, it is easy to process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy5h8NMXVU0m",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The data is organized as follow: \n",
        "\n",
        "* raw: to store the raw data\n",
        "* staging: intermediary step, to explore data\n",
        "* transformed: data ready to be used in a model\n",
        "* predicted: predicted data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKx1eQfJsKLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mDu77cJsLOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0hJPmd1Sh56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "list_files = os.listdir(\"/content/drive/My Drive/Data/Dataroots_nmbs/raw\")\n",
        "for files in sorted(list_files):\n",
        "  print (files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAyYQ697DGmf",
        "colab_type": "text"
      },
      "source": [
        "## Weather data\n",
        "![alt text](https://openweathermap.org/img/w/10d.png)\n",
        "\n",
        "\n",
        "\n",
        "Directly, we notice that the format is json lines files. We can even verify the content of one of the files.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNvyNyufOtFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_weather_files = [\"/content/drive/My Drive/Data/Dataroots_nmbs/raw/{}\".format(file)\n",
        "                      for file in list_files if \"owm\" in file]\n",
        "print(list_weather_files)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9vyAT0hO6De",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We need to open these JSON lines and normalise them. Let's try with one example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KReccZyOO46z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pandas.io.json import json_normalize\n",
        "\n",
        "df_weather = pd.DataFrame()\n",
        "with open(list_weather_files[0]) as f:\n",
        "  for line in f:\n",
        "    line_ = json.loads(line)\n",
        "    df_weather = pd.concat([df_weather, json_normalize(line_)])\n",
        "\n",
        "df_weather.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8YtrwwKzeHb",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAQyItrGPr54",
        "colab_type": "text"
      },
      "source": [
        "we also see that there are 2 id's, one for the weather and one for the other  general data, hence we should rename the weather data before joining it with  the other data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tiix67X-CWgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pandas.io.json import json_normalize\n",
        "\n",
        "\n",
        "\n",
        "df_weather = pd.DataFrame()\n",
        "with open(list_weather_files[0]) as f:\n",
        "  for line in f:\n",
        "    line_ = json.loads(line)\n",
        "    temp_weather = json_normalize(line_, \"weather\")\n",
        "    temp_weather[\"key\"] = 1\n",
        "    temp_weather.rename({\"id\": \"id_weather\"}, axis=1, inplace=True)\n",
        "    temp_other = json_normalize(line_)\n",
        "    temp_other[\"key\"] = 1\n",
        "    df_weather = pd.concat([df_weather, temp_weather.merge(temp_other, on=\"key\")])\n",
        "\n",
        "df_weather.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6RuzA-SSXFb",
        "colab_type": "text"
      },
      "source": [
        "We now need to drop the weather column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQRTek_zILhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_weather.drop(\"weather\", axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYgSBUIuaWzJ",
        "colab_type": "text"
      },
      "source": [
        " This is also the moment to do some renaming of the data columns, eg: removing the space, . and other characters, and lowering the string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGumWZQpabEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "print(\"columns\", df_weather.columns)\n",
        "\n",
        "\n",
        "def clean_col(col):\n",
        "  \"\"\"Clean a string.\n",
        "\n",
        "  In our case, the column names of the dataframe. We remove all the characters \n",
        "  that are not a digit (0-9) nor a letter (a-z) not capital letter (A-Z) and \n",
        "  and replace them by a underscore _\n",
        "  @params col : string - string to clean\n",
        "  \"\"\"\n",
        "  return re.sub(r\"[^0-9a-zA-Z]+\", \"_\", col.rstrip()).lower()\n",
        "\n",
        "\n",
        "df_weather.columns = [clean_col(col) for col in df_weather.columns]\n",
        "print(\"after renaming\", df_weather.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXCih5zWkJ7p",
        "colab_type": "text"
      },
      "source": [
        "#### Schema\n",
        "\n",
        "We need to make sure that our data always has the same types. Indeed if we receive update of the data it should always have the same data types. It could be that the data is malformed and does not have the same type, then there is a risk that the rest of the code fails. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0KnLbSfkHAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_weather.dtypes)\n",
        "# let us save the schema and use it for the other datasets\n",
        "df_weather.dtypes.reset_index().to_csv(\"/content/drive/My Drive/Data/Dataroots_nmbs/staging/schema_weather.csv\",\n",
        "                                       header=[\"variable\", \"type\"], index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEXA9Pu1o5wf",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c39u-FeATxDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import datetime as dt\n",
        "\n",
        "print(list_weather_files[0])\n",
        "\n",
        "# the date is exactly 8 figures and the time is 6, we can extract it with a\n",
        "# regular expression\n",
        "\n",
        "list_time_date = re.findall(r\"([0-9]{8})_([0-9]{6})\", list_weather_files[0])\n",
        "date_time = ':'.join(list_time_date[0])\n",
        "print(date_time)\n",
        "\n",
        "# we just need to have it in the right format of the date\n",
        "\n",
        "date = dt.datetime.strptime(date_time, \"%Y%m%d:%H%M%S\") + dt.timedelta(hours=1)\n",
        "\n",
        "# little problem is that the hour of the day is the English one, beeing one\n",
        "# hour earlier than the Brussels time\n",
        "print(date)\n",
        "\n",
        "\n",
        "def get_date(string, hour):\n",
        "  \n",
        "  list_time_date = re.findall(r\"([0-9]{8})_([0-9]{6})\", string)\n",
        "  date_time = ':'.join(list_time_date[0])\n",
        "  date = dt.datetime.strptime(date_time, \"%Y%m%d:%H%M%S\")\n",
        "  + dt.timedelta(hours=hour)\n",
        "  return date\n",
        "\n",
        "\n",
        "df_weather[\"date\"] = get_date (list_weather_files[0], 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3VGMJgQ1khK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df_weather.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrbCMPjw0PZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_weather.dtypes.reset_index().to_csv (\"/content/drive/My Drive/Data/Dataroots_nmbs/staging/schema_owm.csv\",\n",
        "                                       header=[\"variable\", \"type\"], index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lKKxqIYT_Q8",
        "colab_type": "text"
      },
      "source": [
        "Combining all the above steps into a function, that can be applied to a file and then returns a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbaUL393XOuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_weather_data(file_name, schema):\n",
        " \n",
        "  df_weather = pd.DataFrame()\n",
        "  with open(file_name) as f:\n",
        "    for line in f:\n",
        "      line_ = json.loads(line)\n",
        "      # flatten the weather data\n",
        "      temp_weather = json_normalize(line_, \"weather\")\n",
        "      temp_weather.rename({\"id\": \"id_weather\"}, axis=1, inplace=True)\n",
        "      temp_weather[\"key\"] = 1\n",
        "      # normalize the other data\n",
        "      temp_other = json_normalize(line_)\n",
        "      temp_other[\"key\"] = 1\n",
        "      # join the weather and other data\n",
        "      df_weather = pd.concat([df_weather, temp_weather.merge(temp_other, on=\"key\")])\n",
        "  # remove the weather data\n",
        "  df_weather_final = df_weather.drop(\"weather\", axis=1)\n",
        "  # cleaning the column names\n",
        "  df_weather_final.columns = [clean_col(col) for col in df_weather_final.columns]\n",
        "  # add the datetime\n",
        "  df_weather_final[\"date\"] = get_date(file_name, 1)\n",
        "  # verifying the schema\n",
        "  for i, col in enumerate(schema.variable):\n",
        "    df_weather_final[col] = df_weather_final[col].astype(schema.iloc[i, :].type)\n",
        "  return df_weather_final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yew8JngR1XhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fJwQnDWZkzK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now that we have the function, we can apply it to all the dataframes and concatenate all the data together, to have a clean dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTqUhsFzZbIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_weather = pd.DataFrame()\n",
        "schema = pd.read_csv(\"/content/drive/My Drive/Data/Dataroots_nmbs/staging/schema_owm.csv\")\n",
        "for file in list_weather_files:\n",
        "  print(file)\n",
        "  df_weather = pd.concat([df_weather, get_weather_data(file, schema)])\n",
        "\n",
        "df_weather.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q55JqttlyLba",
        "colab_type": "text"
      },
      "source": [
        "## NMBS data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbyqhRXvtHuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_nmbs_files = [\"/content/drive/My Drive/Data/Dataroots_nmbs/raw/{}\".format(file)\n",
        "                   for file in list_files if \"nmbs\" in file]\n",
        "print(list_nmbs_files)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Re8zuENtWya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_nmbs = pd.read_csv(list_nmbs_files[0])\n",
        "df_nmbs.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRWn23QjXpMF",
        "colab_type": "text"
      },
      "source": [
        "A frequent mistake with using python pandas is to save the index, when there is no index. Then pandas create an additional column named 'unnamed', which should be dropped. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1RJMzHIdVDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_nmbs.drop(['Unnamed: 0', 'stop_name', 'route_id', 'service_id', 'route_long_name' ], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-_U8AmCgHTa",
        "colab_type": "text"
      },
      "source": [
        "The rest of the data looks good. \n",
        "First we create the schema as previously and then, we will concatenate and explore in a later phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ-OhAGatk8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_nmbs.columns = [clean_col(col) for col in df_nmbs.columns]\n",
        "\n",
        "# let us save the schema and use it for the other datasets\n",
        "df_nmbs.dtypes.reset_index().to_csv(\"/content/drive/My Drive/Data/Dataroots_nmbs/staging/schema_nmbs_.csv\",\n",
        "                                    header=[\"variable\", \"type\"], index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6SnCQjhuAjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_nmbs_data(file_name, schema):\n",
        "  \n",
        "  df_nmbs = pd.read_csv(file_name)\n",
        "  df_nmbs.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
        "  # cleaning the column names\n",
        "  df_nmbs.columns = [clean_col(col) for col in df_nmbs.columns]\n",
        "  for i, col in enumerate(schema.variable):\n",
        "    df_nmbs[col] = df_nmbs[col].astype(schema.iloc[i,:].type)\n",
        "  # add the datetime\n",
        "  df_nmbs[\"date\"] = get_date(file_name, 1)\n",
        "  return df_nmbs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwO4BiGFnFkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_nmbs = pd.DataFrame()\n",
        "schema = pd.read_csv(\"/content/drive/My Drive/Data/Dataroots_nmbs/staging/schema_nmbs_.csv\")\n",
        "for file in list_nmbs_files:\n",
        "  df_nmbs = pd.concat([df_nmbs, get_nmbs_data(file, schema)])\n",
        "df_nmbs.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}